bert源码阅读
-理清：
	模型输入
	模型输入转换成向量的过程
	模型结构
	模型输出
	分类任务，fine-tuning微调，调的是什么？
	其它任务的微调怎么做？
	中文的词向量和句向量如何获取？
	

1、直接使用训练好的bert去实现MRPC任务
MRPC任务，是一个二分类任务，判断两个句子意思是否相同，相同则为1，不同则为0
	train数据格式如下
		Quality	#1 ID	#2 ID	#1 String	#2 String
		1	702876	702977	Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .	Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .
		0	2108705	2108831	Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .	Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .
	test数据格式如下，注意到这是没有标签的数据，index只是索引
		index	#1 ID	#2 ID	#1 String	#2 String
		0	1089874	1089925	PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .	Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .
		1	3019446	3019327	The world 's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected .	Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .
		2	1945605	1945824	According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 .	The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 .
数据有了，预训练模型也能下载，bert源码也能下载（其中就包含了有关这个任务的接口），那么怎么去训练和测试这个任务呢？
（1）首先，要配置环境变量
	BERT_BASE_DIR和GLUE_DIR/MRPC，前者是下载好了预训练模型的路径，后者是数据集路径
（2）然后，运行代码
	python run_classifier.py \
		--task_name=MRPC \
		--do_train=true \
		--do_eval=true \
		--data_dir=$GLUE_DIR/MRPC \
		--vocab_file=$BERT_BASE_DIR/vocab.txt \
		--bert_config_file=$BERT_BASE_DIR/bert_config.json \
		--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
		--max_seq_length=128 \
		--train_batch_size=8 \
		--learning_rate=2e-5 \
		--num_train_epochs=3.0 \
		--output_dir=/tmp/mrpc_output/
	其中，接口就写在run_classifier.py 中，指定task_name=MRPC就能去执行这个任务，之后当我们要去做其他分类任务时，一般做法是复制一份MRPC类的代码，在其中微调即可。
	这里最常见的问题就是内存不够，通常我们的GPU只有8G作用的显存，因此对于小的模型(bert-base)，我们最多使用batchsize=8，而如果要使用bert-large，那么batchsize只能设置成1。
（3）最后，结果如下
	***** Eval results *****
	eval_accuracy = 0.845588
	eval_loss = 0.505248
	global_step = 343
	loss = 0.505248

2、读run_classifier.py
（1）首先，是各种配置，这些配置都可以在上述的run命令里面去指定，其中有一些是必须指定的，可以看到main入口中规定了：
			flags.mark_flag_as_required("data_dir")
			flags.mark_flag_as_required("task_name")
			flags.mark_flag_as_required("vocab_file")
			flags.mark_flag_as_required("bert_config_file")
			flags.mark_flag_as_required("output_dir")
（2）其次，是各种类，主要可以分成两大类
	（2.1）数据处理类
			InputExample
			PaddingInputExample
			InputFeatures
			DataProcessor
	（2.2）下游任务类
			XnliProcessor
			MnliProcessor
			MrpcProcessor
			ColaProcessor
（3）最后，是各种方法
			file_based_convert_examples_to_features
			file_based_input_fn_builder
			_truncate_seq_pair
			create_model
			model_fn_builder
			input_fn_builder
			convert_examples_to_features

3、趟一遍MRPC任务的执行过程
这个过程就写在main方法中，主要有：
（1）设计日志级别. 控制哪些日志打印到屏幕上。
	tf.logging.set_verbosity(tf.logging.INFO)
基本使用方法就是：
	tf.logging.info("*** This is an example ***")
（2）检查你下载的预训练模型和参数do_lower_case是否相匹配，默认do_lower_case是True，表示要匹配uncased模型（不区分大小写，模型的vocab文件中的单词只有小写）
	tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,
                                                FLAGS.init_checkpoint)
（3）加载mrpc类，获取该分类任务的标签列表，将train文件转换成train_examples列表，其中每个元素都是包含4个属性的InputExample对象
	processors = {
		"cola": ColaProcessor,
		"mnli": MnliProcessor,
		"mrpc": MrpcProcessor,
		"xnli": XnliProcessor,
	}
	processor = processors[task_name]() # 创建mrpc类对象

	label_list = processor.get_labels() # 返回当前任务的标签列表，该任务只有0,1，1表示两个句子意思相同
	
	def get_labels(self):
		return ["0", "1"]
		
	train_examples = processor.get_train_examples(FLAGS.data_dir)  # 十分关键的一步，文件是怎么转换成模型输入的
	
	def get_train_examples(self, data_dir):
		return self._create_examples(
		self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")  # 读取文件

	def _create_examples(self, lines, set_type): 
	'''
	文件中的每一行都会作为一个InputExample对象，具有4个属性
	比如对于train文件的第一行来说：guid = train-0 ， text_a = line[3], text_b = line[4], label=line[0]
	convert_to_unicode 是为了兼容Python2和Python3，因为Python3的str就是unicode，
	而Python2的str其实是bytearray，Python2却有一个专门的unicode类型
	
	总之，文件在经过了这个方法后，会返回一个examples列表，其中每个元素都是包含4个属性的InputExample对象
	'''
	
		examples = []
		for (i, line) in enumerate(lines):
			if i == 0:
				continue
			guid = "%s-%s" % (set_type, i)
			text_a = tokenization.convert_to_unicode(line[3])
			text_b = tokenization.convert_to_unicode(line[4])
			if set_type == "test":
				label = "0"
			else:
				label = tokenization.convert_to_unicode(line[0])
			examples.append(
			  InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
		return examples
	
（4）加载FullTokenizer类，
	tokenizer = tokenization.FullTokenizer(
		vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)
	（4.1）这里，首先要说明vocab是词表文件，假设使用了uncase模型（不区分大小写，即模型下的vocab文件只有小写的单词），
	那么do_lower_case就要设置成true；
	（4.2）然后，tokenizer对象的作用，其实例化的方法：
	def __init__(self, vocab_file, do_lower_case=True):
		self.vocab = load_vocab(vocab_file)  # 返回词典，即dict对象 {词：index}
		self.inv_vocab = {v: k for k, v in self.vocab.items()} # {index: 词}
		self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
		self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)
	
	
	

4、梳理上述的过程
（1）从file到模型输入

（2）模型结构

（3）从模型输入到模型输出






